{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to combine main city data file with AEA file\n",
    "def aeaMerge(file1, file2):  # File1 should probably be the CD_CSV file mostly\n",
    "    df_file1 = pd.DataFrame(pd.read_csv(file1, sep=',', encoding='utf-8'))\n",
    "    df_file2 = pd.DataFrame(pd.read_csv(file2, sep=',', encoding='utf-8'))\n",
    "\n",
    "    # Check to make sure correct second file was passed in\n",
    "    if file2 == 'files/Fully_Cleaned_AEA_Data.csv':\n",
    "        # By making this new DataFrame I cut out the excess index column from the second file.\n",
    "        stats_from_file_two = pd.DataFrame(data=[df_file2['County'], df_file2['Higher Degree'], df_file2['H.S Diploma'], df_file2['No H.S Diploma']]).transpose()\n",
    "        df_output = pd.merge(df_file1, stats_from_file_two, on='County')\n",
    "        df_output.to_csv('files/CD_and_Statistical_Atlas_Data_Combined.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to combine all of the zip code level data (from cities and counties) into the main City Data csv file\n",
    "def combinezips(file1, file2):\n",
    "    # Create dfs for the two zip code related files\n",
    "    df_file1 = pd.DataFrame(pd.read_csv(file1, sep=',', encoding='utf-8'))\n",
    "    df_file2 = pd.DataFrame(pd.read_csv(file2, sep=',', encoding='utf-8'))\n",
    "\n",
    "    df_file2['ZipCode'] = df_file2['ZipCode'].astype('int32')\n",
    "\n",
    "    # Merge two dfs and then create a new sub-dataframe that contains just the columns we care about\n",
    "    combined = pd.merge(df_file1, df_file2, on='ZipCode')\n",
    "    combined_main_columns = pd.DataFrame(data=[combined['City'], combined['ZipCode'], combined['Lat'], combined['Long']]).transpose()\n",
    "    combined_main_columns['ZipCode'] = combined_main_columns['ZipCode'].astype('int32')\n",
    "\n",
    "    # Load in the main file so it can have zip code data merged into it\n",
    "    main_file = pd.DataFrame(pd.read_csv('files/CD_and_Statistical_Atlas_Data_Combined.csv', sep=',', encoding='utf-8'))\n",
    "    main_file = main_file.rename(index=str, columns={\"City Name\": \"City\"})\n",
    "\n",
    "    updated_main = pd.merge(main_file, combined_main_columns, on='City')\n",
    "\n",
    "    # Code to deal with smaller cities that didn't get a zip code on web site we scrapped\n",
    "    # Resolved by giving them county level lat/long and zip\n",
    "    blank_subset = pd.merge(main_file, combined_main_columns, on='City', how='left')\n",
    "    blank_subset['ZipCode'] = blank_subset['ZipCode'].fillna(0)\n",
    "    blank_subset = blank_subset[blank_subset['ZipCode'] == 0]\n",
    "\n",
    "    # Load in the county zip files to fill in the gaps\n",
    "    county_zip1 = pd.DataFrame(pd.read_csv('files/County_Zip.csv', sep=',', encoding='utf-8'))\n",
    "    county_zip2 = pd.DataFrame(pd.read_csv('files/County_Zip_Lat_Long.csv', sep=',', encoding='utf-8'))\n",
    "    county_zip2['ZipCode'] = county_zip2['ZipCode'].astype('int32')\n",
    "    combined_county_zip = pd.merge(county_zip1, county_zip2, on='ZipCode')\n",
    "    combined_county_zip['County'] = combined_county_zip['County'] + '-County'\n",
    "    blank_subset_filled = pd.merge(blank_subset, combined_county_zip, on='County')\n",
    "\n",
    "    # Clean out 2 useless columns\n",
    "    updated_main = updated_main.drop(updated_main.columns[0], axis=1)\n",
    "    updated_main = updated_main.drop(updated_main.columns[0], axis=1)\n",
    "    blank_subset_filled = blank_subset_filled.drop(blank_subset_filled.columns[0], axis=1)\n",
    "    blank_subset_filled = blank_subset_filled.drop(blank_subset_filled.columns[0], axis=1)\n",
    "\n",
    "    # Clean out additional junk columns that were produced by merge calls\n",
    "    del blank_subset_filled['Unnamed: 0_y']\n",
    "    del blank_subset_filled['Unnamed: 0_x']\n",
    "    del blank_subset_filled['Long_x']\n",
    "    del blank_subset_filled['Lat_x']\n",
    "    del blank_subset_filled['ZipCode_x']\n",
    "\n",
    "    # Reformatted column types\n",
    "    blank_subset_filled = blank_subset_filled.rename(index=str, columns={'Lat_y': 'Lat', 'Long_y': 'Long', 'ZipCode_y': 'ZipCode'})\n",
    "    blank_subset_filled['ZipCode'] = blank_subset_filled['ZipCode'].astype('int32')\n",
    "    blank_subset_filled['Lat'] = blank_subset_filled['Lat'].astype('object')\n",
    "    blank_subset_filled['Long'] = blank_subset_filled['Long'].astype('object')\n",
    "\n",
    "    df = [updated_main, blank_subset_filled]\n",
    "\n",
    "    # Concatenate together cities that got zip code from city-zip file and county-zip file\n",
    "    final_dataframe = pd.concat(df)\n",
    "    final_dataframe.to_csv('files/CD_SA_with_Zip_all.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to combine all of the zip code level data (from cities and counties) into the main City Data csv file\n",
    "def combinezips(file1, file2):\n",
    "    # Create dfs for the two zip code related files\n",
    "    df_file1 = pd.DataFrame(pd.read_csv(file1, sep=',', encoding='utf-8'))\n",
    "    df_file2 = pd.DataFrame(pd.read_csv(file2, sep=',', encoding='utf-8'))\n",
    "\n",
    "    df_file2['ZipCode'] = df_file2['ZipCode'].astype('int32')\n",
    "\n",
    "    # Merge two dfs and then create a new sub-dataframe that contains just the columns we care about\n",
    "    combined = pd.merge(df_file1, df_file2, on='ZipCode')\n",
    "    combined_main_columns = pd.DataFrame(data=[combined['City'], combined['ZipCode'], combined['Lat'], combined['Long']]).transpose()\n",
    "    combined_main_columns['ZipCode'] = combined_main_columns['ZipCode'].astype('int32')\n",
    "\n",
    "    # Load in the main file so it can have zip code data merged into it\n",
    "    main_file = pd.DataFrame(pd.read_csv('files/CD_and_Statistical_Atlas_Data_Combined.csv', sep=',', encoding='utf-8'))\n",
    "    main_file = main_file.rename(index=str, columns={\"City Name\": \"City\"})\n",
    "\n",
    "    updated_main = pd.merge(main_file, combined_main_columns, on='City')\n",
    "\n",
    "    # Code to deal with smaller cities that didn't get a zip code on web site we scrapped\n",
    "    # Resolved by giving them county level lat/long and zip\n",
    "    blank_subset = pd.merge(main_file, combined_main_columns, on='City', how='left')\n",
    "    blank_subset['ZipCode'] = blank_subset['ZipCode'].fillna(0)\n",
    "    blank_subset = blank_subset[blank_subset['ZipCode'] == 0]\n",
    "\n",
    "    # Load in the county zip files to fill in the gaps\n",
    "    county_zip1 = pd.DataFrame(pd.read_csv('files/County_Zip.csv', sep=',', encoding='utf-8'))\n",
    "    county_zip2 = pd.DataFrame(pd.read_csv('files/County_Zip_Lat_Long.csv', sep=',', encoding='utf-8'))\n",
    "    county_zip2['ZipCode'] = county_zip2['ZipCode'].astype('int32')\n",
    "    combined_county_zip = pd.merge(county_zip1, county_zip2, on='ZipCode')\n",
    "    combined_county_zip['County'] = combined_county_zip['County'] + '-County'\n",
    "    blank_subset_filled = pd.merge(blank_subset, combined_county_zip, on='County')\n",
    "\n",
    "    # Clean out 2 useless columns\n",
    "    updated_main = updated_main.drop(updated_main.columns[0], axis=1)\n",
    "    updated_main = updated_main.drop(updated_main.columns[0], axis=1)\n",
    "    blank_subset_filled = blank_subset_filled.drop(blank_subset_filled.columns[0], axis=1)\n",
    "    blank_subset_filled = blank_subset_filled.drop(blank_subset_filled.columns[0], axis=1)\n",
    "\n",
    "    # Clean out additional junk columns that were produced by merge calls\n",
    "    del blank_subset_filled['Unnamed: 0_y']\n",
    "    del blank_subset_filled['Unnamed: 0_x']\n",
    "    del blank_subset_filled['Long_x']\n",
    "    del blank_subset_filled['Lat_x']\n",
    "    del blank_subset_filled['ZipCode_x']\n",
    "\n",
    "    # Reformatted column types\n",
    "    blank_subset_filled = blank_subset_filled.rename(index=str, columns={'Lat_y': 'Lat', 'Long_y': 'Long', 'ZipCode_y': 'ZipCode'})\n",
    "    blank_subset_filled['ZipCode'] = blank_subset_filled['ZipCode'].astype('int32')\n",
    "    blank_subset_filled['Lat'] = blank_subset_filled['Lat'].astype('object')\n",
    "    blank_subset_filled['Long'] = blank_subset_filled['Long'].astype('object')\n",
    "\n",
    "    df = [updated_main, blank_subset_filled]\n",
    "\n",
    "    # Concatenate together cities that got zip code from city-zip file and county-zip file\n",
    "    final_dataframe = pd.concat(df)\n",
    "    final_dataframe.to_csv('files/CD_SA_with_Zip_all.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to combine all the remaining educational data into our master file\n",
    "def male_female_age_edu_combiner():\n",
    "    medu = pd.DataFrame(pd.read_csv('files/ADEASexRatioMale.csv', sep=',', encoding='utf-8'))\n",
    "    fedu = pd.DataFrame(pd.read_csv('files/ADEASexRatioFemale.csv', sep=',', encoding='utf-8'))\n",
    "    aedu = pd.DataFrame(pd.read_csv('files/AllBachDegreeByAge.csv', sep=',', encoding='utf-8'))\n",
    "\n",
    "    # Grab the columns for one last cleaning\n",
    "    mcolumns = medu.columns\n",
    "    fcolumns = fedu.columns\n",
    "    acolumns = aedu.columns\n",
    "\n",
    "    # Loop through (skipping the first column) and remove % signs from the data for each data frame\n",
    "    counter = 0\n",
    "    for col in mcolumns:\n",
    "        if counter == 1:\n",
    "            medu[col] = medu[col].str.replace('%', '')\n",
    "        else:\n",
    "            counter += 1\n",
    "    counter = 0\n",
    "    for col in fcolumns:\n",
    "        if counter == 1:\n",
    "            fedu[col] = fedu[col].str.replace('%', '')\n",
    "        else:\n",
    "            counter += 1\n",
    "    counter = 0\n",
    "    for col in acolumns:\n",
    "        if counter == 1:\n",
    "            aedu[col] = aedu[col].str.replace('%', '')\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "    # Load in enthnicity.csv which is our most complete file at this point\n",
    "    masterfile = pd.DataFrame(pd.read_csv('files/ethnicity.csv', sep=',', encoding='utf-8'))\n",
    "\n",
    "    # Merge int additional data\n",
    "    masterfile = pd.merge(masterfile, medu, on='County')\n",
    "    masterfile = pd.merge(masterfile, fedu, on='County')\n",
    "    masterfile = pd.merge(masterfile, aedu, on='County')\n",
    "\n",
    "    # Remove unneeded columns\n",
    "    masterfile = masterfile.drop(masterfile.columns[0], axis=1)\n",
    "    masterfile = masterfile.drop('Unnamed: 0_y', axis=1)\n",
    "    masterfile.to_csv('files/Master_Data_File.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to combine the two school related data sources together / not in use actually\n",
    "def combine_school_data():\n",
    "    file1 = pd.DataFrame(pd.read_csv('Summarized_School_Scores.csv', sep=',', encoding='utf-8'))\n",
    "    file2 = pd.DataFrame(pd.read_csv('School_Safety_Data.csv', sep=',', encoding='utf-8'))\n",
    "\n",
    "    df = pd.merge(file1, file2, on='School Number')\n",
    "    # Drop unneeded columns\n",
    "    df = df.drop(df.columns[0:2], axis=1)\n",
    "    df = df.drop(df.columns[6:8], axis=1)\n",
    "    df.to_csv('School_Data_Combined.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    txt = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(text):\n",
    "    if 'White alone' in text:\n",
    "        txt = text.split('White alone ')[1]\n",
    "        txtw = txt.split(' ')[0]\n",
    "    else:\n",
    "        txtw = '0'\n",
    "    if 'Hispanic' in text:\n",
    "        txt = text.split('Hispanic ')[1]\n",
    "        txth = txt.split(' ')[0]\n",
    "    else:\n",
    "        txth = '0'\n",
    "    if 'Black alone' in text:\n",
    "        txt = text.split('Black alone ')[1]\n",
    "        txtb = txt.split(' ')[0] \n",
    "    else:\n",
    "        txtb = '0'\n",
    "    if 'Asian alone' in text: \n",
    "        txt = text.split('Asian alone ')[1]\n",
    "        txta = txt.split(' ')[0]\n",
    "    else:\n",
    "        txta = '0'\n",
    "    if 'American Indian alone' in text:\n",
    "        txt = text.split('American Indian alone ')[1]\n",
    "        txtai = txt.split(' ')[0]\n",
    "    else:\n",
    "        txtai = '0'\n",
    "    if 'Two or more races' in text:\n",
    "        txt = text.split('Two or more races ')[1]\n",
    "        txttm = txt.split(' ')[0]\n",
    "    else:\n",
    "        txttm = '0'\n",
    "    if 'Other race alone' in text:\n",
    "        txt = text.split('Other race alone ')[1]\n",
    "        txtoth = txt.split(' ')[0]\n",
    "    else:\n",
    "        txtoth = '0'\n",
    "    if 'Native Hawaiian and Other Pacific Islander alonebr' in text:\n",
    "        txt = text.split('Native Hawaiian and Other Pacific Islander alonebr ')[1]\n",
    "        txtnh = txt.split(' ')[0]\n",
    "    elif 'Native Hawaiian and Other Pacific Islander alone' in text:\n",
    "        txt = text.split('Native Hawaiian and Other Pacific Islander alone ')[1]\n",
    "        txtnh = txt.split(' ')[0]\n",
    "    elif 'Native Hawaiian and Other  Pacific Islander alone' in text:\n",
    "        txt = text.split('Native Hawaiian and Other  Pacific Islander alone ')[1]\n",
    "        txtnh = txt.split(' ')[0]   \n",
    "    else:\n",
    "        txtnh ='0'\n",
    "    return txtw, txth, txtb, txta, txtai, txttm, txtoth, txtnh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hey Kaurik you need to methodize your code here\n",
    "def extract_ethnicity():\n",
    "    # Import Pandas\n",
    "    import pandas as pd \n",
    "    # Read File to extract columns for ethinic data \n",
    "    data = pd.read_csv('files/CD_SA_with_Zip_all.csv')\n",
    "    # Import numpy \n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    # Set all Ethnic columns to null\n",
    "    data['White alone'] = np.nan\n",
    "    data['Hispanic'] = np.nan\n",
    "    data['Two or more races'] = np.nan\n",
    "    data['Native Hawaiian'] = np.nan\n",
    "    data['Black alone'] = np.nan\n",
    "    data['Asian alone'] = np.nan\n",
    "    data['American Indian alone'] = np.nan\n",
    "    data['Other race alone'] = np.nan\n",
    "    \n",
    "    # dropping impure rows\n",
    "    data.drop(data.index[np.where(data['Racial Breakdown'] == '*')], inplace=True)\n",
    "    # Deleting the initial additional index \n",
    "    del data['Unnamed: 0']\n",
    "    # Create a copy \n",
    "    data1 = data.copy()\n",
    "    # Reset index\n",
    "    data1.reset_index(inplace=True)\n",
    "    \n",
    "    i=0\n",
    "    for index, row in data.iterrows():\n",
    "        if i==1641:\n",
    "            break\n",
    "        line = row['Racial Breakdown']\n",
    "        clean_text = remove_punct(line)\n",
    "        w, h, b, a, ai, tm, oth, nh = extract(clean_text)\n",
    "        data1.at[i, 'White alone'] = w\n",
    "        data1.at[i, 'Hispanic'] = h\n",
    "        data1.at[i, 'Two or more races'] = tm\n",
    "        data1.at[i, 'Native Hawaiian'] = nh\n",
    "        data1.at[i, 'Black alone'] = b\n",
    "        data1.at[i, 'Asian alone'] = a\n",
    "        data1.at[i, 'American Indian alone'] = ai\n",
    "        data1.at[i, 'Other race alone'] = oth\n",
    "        i = i + 1\n",
    "    \n",
    "    # Remove additional index column\n",
    "    del data1['index']\n",
    "    \n",
    "    # Create a CSV for further processing\n",
    "    data1.to_csv('files/ethnicity.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to update what files you are passing in for combination. At this point,\n",
    "# CD_and_Statistical_Atlas_Data_Combined is the master file currently\n",
    "if __name__ == '__main__':\n",
    "    aeaMerge('files/Fully_Cleaned_CD_Data.csv', 'files/Fully_Cleaned_AEA_Data.csv')\n",
    "    combinezips('files/City_Zip.csv', 'files/City_Zip_Lat_Long.csv')\n",
    "    \n",
    "    extract_ethnicity()\n",
    "    \n",
    "    male_female_age_edu_combiner()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
